\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{xspace}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{subfig}
\usepackage{fullpage}
\usepackage{url,paralist}

\author{Philip Pickering\\ \url{pgpick@gmx.at} \and Marco Eilers\\ \url{eilers.marco@googlemail.com} \and Thomas Bracht Laumann Jespersen\\ \url{ntl316@alumni.ku.dk}}
\title{Statistical Methods for Machine Learning\\ Assignment 2: Basic Learning Algorithms}
\date{}

\newcommand{\vect}[1]{\ensuremath{\boldsymbol{\mathbf{#1}}}\xspace}
%%\newcommand{\M}{\ensuremath{\begin{pmatrix}100 & 0 

\newcommand{\knollA}{\textsc{knollA}\xspace}
\newcommand{\knollB}{\textsc{knollB}\xspace}
\newcommand{\knollC}{\textsc{knollC}\xspace}

%% %% Uncomment these lines to get a box around all figures
%% \usepackage{float}
%% \floatstyle{boxed}
%% \restylefloat{figure}

\begin{document}
\maketitle

\section{Regression}

\subsection{Maximum Likelihood solution}

Use linear model
\[
y(\vect{x},\vect{w}) = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_D x_D
\]
and for the $D$ variables we let $\phi_i(\vect{x}) = x_i$ for
$i = 1,\dots,D$ and $\phi_0(\vect{x}) = 1$. 
\subsubsection{Selection 1}

For our first selection $S_1$ our design matrix becomes a $200\times
5$ matrix.

\[
\vect{\Phi}_{S_1} = \begin{bmatrix}1 & \vect{x}_{1,1} & \vect{x}_{1,2} & \vect{x}_{1,3} & \vect{x}_{1,4}\\
\hfill & \hfill & \vdots & \hfill & \hfill\\
1 & \vect{x}_{i,1} & \vect{x}_{i,2} & \vect{x}_{i,3} & \vect{x}_{i,4}\\
\hfill & \hfill & \vdots & \hfill & \hfill\\
1 & \vect{x}_{N,1} & \vect{x}_{N,2} & \vect{x}_{N,3} & \vect{x}_{N,4}\\
\end{bmatrix}
\]
where the notation $\vect{x}_{i,j}$ indicates the $j$'th entry in the
$i$'th vector. %%Similarly we obtain a design matrix for our second
%%selection, $S_2$ of dimensions $200\times 2$, with $D = 2$:

Finding the ML estimate of our parameters for $S_1$ gives
\[
\vect{w}_{S_1} = \left[\!\begin{tabular}{r@{.}l} -43 & 0947\\ -0 &
  1299\\ 0 & 0352\\ 0 & 9335\\ -0 &
  0433\end{tabular}\!\right]\quad\text{ and }\quad \mathrm{RMS}_{S_1} = 4.3897
\]

\subsubsection{Selection 2}

Our second selection $S_2$ consists only of the data from the `Abdomen
2' column, giving a design matrix $\vect{\Phi}_{S_2}$ of dimensions
$200\times 2$. Training the model on the same training data yields:
\[
\vect{w}_{S_2} = \left[\!\begin{tabular}{r@{.}l}
  -37 & 4085\\
  0 & 6133
  \end{tabular}\!\right]\quad
\text{ and }\quad \mathrm{RMS}_{S_2} = 5.2064
\]

%% Conclusion
\subsubsection{Discussion}
Just looking at the root mean square values of the two selections, it
appears that $S_1$ performs better than $S_2$, but not by a lot. This
could suggest that either the variable `Abdomen 2' is the most descriptive
in terms of body fat, or that the linear model simply is a poor fit no
matter how many variables we include. It could be a combination of the
two.

It could probably be argued that the linear regression model is a poor
predictor, but including more variables should improve the results.

\subsection{Maximum a posteriori solution}

%% Use MAP estimation. Fix prior to zero mean isotropic Gaussian (eq. 3.52)
%% \[
%% p(\vect{w}|\alpha) = \mathcal{N}(\vect{w}|\vect{0},\alpha^{-1}\vect{I})
%% \]
%% Set noise precision parameter $\beta = 1$. Compute (3.53) and (3.54)
%% \[
%% \text{(3.53)}\quad \vect{m}_N = \beta\vect{S}_N\vect{\Phi}^T\mathsf{t}
%% \]
%% and
%% \[
%% \text{(3.54)}\quad \vect{S}_N^{-1} = \alpha\vect{I} + \beta\vect{\Phi}^T\vect{\Phi}
%% \]

\begin{figure}
\subfloat[\hfill]{\includegraphics[width=.5\textwidth]{src/mapsel1.pdf}}
\subfloat[\hfill]{\includegraphics[width=.5\textwidth]{src/mapsel2.pdf}}
\caption{Plot of RMS against varying values of $\alpha$}
\label{fig:rmsalpha}
\end{figure}

For $S_1$ the lowest root mean square value is 4.3614, the same as the
ML solution differing on the second digit. The difference is larger
for $S_2$ in which we obtain the value 5.1071, differing from the ML
solution on the first digit.

In fig.~\ref{fig:rmsalpha} two plots are found of the root mean square
values for varying values of $\alpha$. In both plots we set $\beta =
1$. The RMS value of the ML solution is plotted as a straight line.

%%Apply model to test set and compute RMS error for different values of the precision parameter $\alpha$.

\subsubsection{Comparison}

Firstly, we can observe for both plots that when $\alpha = 0$, we
obtain the same $\mathrm{RMS}$ error for the MAP estimate as for the
ML solution. This is expected and demonstrates that when our prior
precision parameters are set to zero, the MAP estimate becomes the ML
estimate.

Fig.~\ref{fig:rmsalpha}(a) is the plot for $S_1$, and it can be seen
that the $\mathrm{RMS}_\mathrm{MAP}$ error drops below the
$\mathrm{RMS}_\mathrm{ML}$ in the interval $[-0.091,0]$. In
fig.~\ref{fig:rmsalpha}(b) the plot for $S_2$ similarly gives us that
the $\mathrm{RMS}_\mathrm{MAP}$ error is lower in the interval
$[-0.844,0]$.

The intervals in which the MAP estimate outperforms the ML estimate
are very small, and the obtained difference likewise small.

\subsection{Theory}

Verify result in equation (3.49) for the posterior distribution of the
parameters \vect{w} in the linear basis function in which $\vect{m}_N$
and $\vect{S}_N$ are defined 

\newpage
\section{Linear Discriminant Analysis}
\begin{figure}
  \subfloat[\knollA]{\includegraphics[width=.33\textwidth]{src/knollAtrain.pdf}}
  \subfloat[\knollB]{\includegraphics[width=.33\textwidth]{src/knollBtrain.pdf}}
  \subfloat[\knollC]{\includegraphics[width=.33\textwidth]{src/knollCtrain.pdf}}
  \caption{Visualisation of the training data for each of the \textsc{knoll} problems}
  \label{fig:knolldata}
\end{figure}

\subsection{Observations regarding the data}
\begin{figure}

  \subfloat[as]{\includegraphics[width=.5\textwidth]{src/ldaKnollAtrain.pdf}}
  \subfloat[df]{\includegraphics[width=.5\textwidth]{src/ldaKnollAtest.pdf}}

  \subfloat[as]{\includegraphics[width=.5\textwidth]{src/ldaKnollBtrain.pdf}}
  \subfloat[df]{\includegraphics[width=.5\textwidth]{src/ldaKnollBtest.pdf}}

  \subfloat[as]{\includegraphics[width=.5\textwidth]{src/ldaKnollCtrain.pdf}}
  \subfloat[df]{\includegraphics[width=.5\textwidth]{src/ldaKnollCtest.pdf}}

  \caption{Visualisation of the training data for each of the \textsc{knoll} problems}
  \label{fig:ldaknollplots}
\end{figure}

In fig.~\ref{fig:knolldata} you can see plots of all three training data sets. It is obvious
that while \knollA and \knollC contain well-separable groups of data points, the distributions in \knollB
seem to overlap. We can therefore expect that LDA will perform relatively well on \knollA and \knollB, whereas
the results for \knollB will likely be more erratic. 

\subsection{Results of LDA}
If we run LDA on all three data sets and plot them with their respective decision boundaries (fig.~\ref{fig:ldaknollplots}), we find 
that the algorithm indeed performs well on the A and C sets. In table~\ref{tab:ldaknollerror} we can see that although the error count is greater than zero for all 
training and test sets, which is also implied by the margin always being slightly negative, the number of errors
is relatively low for both training and test sets. The training set performs slightly better than the test set in both 
cases, which is to be expected, but there is not over- or underfitting problem.

For \knollB, however, the results are much worse. The properties of the underlying distribution simply
do not allow them to be separated by a straight line, which results in a high error percentage and a highly negative margin.

Since the distributions in B overlap to a high degree, it is generally not possible to find a function of any kind
to separate both classes accurately. However, a Quadratic Discriminant Analysis might be able to find out that one class
concentrates a lot more in a certain range, whereas the other one has a higher variance and is therefore spread out more widely.
It could therefore assign all points in this range to one class and the rest to the other. While this will still result in a high
error percentage, such an algorithm might actually find the Bayes Optimal Solution.

%% TODO Table of knoll{A,B,C} error rates, for training and test data.
%%         train test
%% knollA    %     %
%% knollB    %     %
%% knollC    %     %
\begin{table}[h!]
  \centering
  \begin{tabular}{l|c|c|c|c|c|c}
    & \multicolumn{2}{c|}{\knollA} & \multicolumn{2}{|c|}{\knollB} & \multicolumn{2}{|c}{\knollC} \\
    & Training & Test & Training & Test & Training & Test \\
    \hline
    Errors & 1\% & 3\% & 40\% & 49\% & 1\% & 3\% \\ 
    Margin & -0.242763 & -0.142026 & -1.629922 & -2.061411 & -0.002435 & -0.001425 \\
  \end{tabular}
  \caption{Error percentages and margins for each of the \textsc{knoll} problems}
  \label{tab:ldaknollerror}
\end{table}

\newpage
\section{Nearest Neighbor Classification}

\subsection{Nearest Neighbor Classification with Euclidian Metric}

The results of the runs of the classifier for different values of $k$
are found in table~\ref{tbl:results1}. The bottom field of each column
indicates the $k$ for which the classifier gave the lowest error rate.

The error rate is computed by adding up the errors made by the
classifier, and then dividing by the test set size. As we have the
actual class of a given point (for both test and training sets), this
is straightforward to implement.

As a general note, classifying the training set itself will always
give us $k = 1$ as the optimal value, which makes sense, because the
closest point to a point in both the training and test sets is the
point itself, hence the distance will always be zero (and the
classification perfect). But it's interesting to see how the
classifier behaves when more than one neighbor has to be
considered. If a point belonging to one class is surrounded by points
in the other class, then it will be misclassified. As can be seen from
the results, the optimal value for $k$ in all the training sets is $3$,
although it should be noted that the error rate for $k = 3,5$ and $7$
in \knollA are all $0.1$.

In \knollA, the error rates are very low ($< 0.05$) for all the values of $k$,
being lowest when $k = 3$. In \knollB and \knollC the error rates
increase dramatically ($\geq 0.19$), for \knollB the lowest error rates are obtained
when $k = 5$ or $7$. In \knollC the best results are unanimously
obtained when $k = 1$.

%% Hand in: classifier source code, results, short discussion.
%%% Table
%%% k    knollA    knollB    knollC
%%% 1      %                   %
%%% 3
%%% 5
%%% 7
%%% 9      %                   %
\begin{table}
  \centering
  \begin{tabular}{c | c|c | c|c | c|c}
    \hfill & \multicolumn{2}{c|}{\knollA} & \multicolumn{2}{c|}{\knollB} & \multicolumn{2}{c}{\knollC}\\
    $k$ & test & train & test & train & test & train\\\hline
    1 & 0.04 &   -  & 0.23 &   -  & 0.20 &   - \\
    3 & 0.02 & 0.01 & 0.20 & 0.13 & 0.32 & 0.16\\
    5 & 0.03 & 0.01 & 0.19 & 0.19 & 0.34 & 0.21\\
    7 & 0.04 & 0.01 & 0.19 & 0.19 & 0.42 & 0.21\\
    9 & 0.03 & 0.02 & 0.23 & 0.20 & 0.46 & 0.33\\\hline\hline
    \hfill & 3 & 3  &   5  &   3  &   1  &   3 
  \end{tabular}
  \caption{Table of results for $k$-NN classifier on all three \textsc{knoll} problems.}
  \label{tbl:results1}
\end{table}

Observations

%% Why it makes sense to compute error rate for the training
We can compute an accuracy for the $k$-NN on the same training data, which for $k = 1$ always gives an error degree of zero, because the nearest neighbour of a given point in the data set is the point itself. But for higher values of $k$, we can get errors for points surrounded by points from the opposite class.

%% TODO Table of knoll{A,B,C} error rates, for respectively the training data and the test data.
%%
%%    knollA     knollB     knollC
%% k  train test train test train test
%% 1    %    %     %     %    %    %
%% .
%% .
%% 9   %     %     %     %    %    %

\subsection{Changing the Metric}

To prove that $d$ is a metric, given
\[
d(\vect{x},\vect{z}) = \|\vect{M}\vect{x} - \vect{M}\vect{z}\|\text{,
  where } \vect{M} = \begin{pmatrix} 100 & 0 \\ 0 & 1\end{pmatrix}
\]
and $\|\cdot\|$ is the standard $L_2$-norm (in $\mathbb{R}^2$), we
need to verify $\forall \vect{x},\vect{y}\in \mathbb{R}^2$ that
\begin{inparaenum}[1)]
  \item $d(\vect{x},\vect{y}) \geq 0$; 
  \item $d(\vect{x},\vect{y}) = 0 \Leftrightarrow \vect{x} = \vect{y}$;
  \item $d(\vect{x},\vect{y}) =  d(\vect{y},\vect{x})$ (symmetry) and
  \item $\forall \vect{x},\vect{y},\vect{z} \in \mathbb{R}^2 : d(\vect{x},z) \leq d(\vect{x},\vect{y}) + d(\vect{y},\vect{z})$.
\end{inparaenum}

\subsubsection{Proof}

We need only to observe that $\vect{M}$ is a projection $m : \mathbb{R}^2 \rightarrow \mathbb{R}^2$, i.e.\ onto $\mathbb{R}^2$ itself, given by $m(\vect{x}) = \vect{M}\vect{x}$. This immediately gives us all the properties we need, because $L_2$ is itself a (complete) metric on $\mathbb{R}^2$.

For instance, if we let $\vect{x},\vect{y},\vect{z} \in \mathbb{R}^2$ and $\vect{x'},\vect{y'},\vect{z'}$ be the result of applying $m$ on $\vect{x},\vect{y},\vect{z}$ respectively, we can prove the triangle inequality:

\begin{align}
\nonumber d(\vect{x},\vect{z}) &= \left\|\vect{M}\vect{x} - \vect{M}\vect{z}\right\|\\
\nonumber &= \left\|\vect{M}\vect{x} - \vect{M}\vect{y} + \vect{M}\vect{y} - \vect{M}\vect{z}\right\|\\
\nonumber &= \left\|(\vect{x'} - \vect{y'}) + (\vect{y'} - \vect{z'})\right\|\\
\nonumber &\leq \left\|\vect{x'} - \vect{y'}\right\| + \left\|\vect{y'} - \vect{z'}\right\|\quad \text{(by property of } L_2 \text{ norm)}\\
  &= d(\vect{x},\vect{y}) + d(\vect{y},\vect{z})
\end{align}

%% \subsubsection{Original proof --- the long way}

%% Let $\vect{x}, \vect{y} \in \mathbb{R}^2$. As it is easier to consider
%% the square of the $L_2$ norm, we will do so:
%% \begin{align}
%% \nonumber  d(\vect{x},\vect{y})^2 &= \left\|\begin{pmatrix} 100 & 0\\ 0 & 1\end{pmatrix}\begin{pmatrix}x_1\\ x_2\end{pmatrix} - \begin{pmatrix}100 & 0\\ 0 & 1\end{pmatrix}\begin{pmatrix}y_1\\ y_2\end{pmatrix}\right\|^2 \\
%%       \nonumber &= \left\| \begin{pmatrix} 100(x_1 - y_1)\\ x_2 - y_2\end{pmatrix}\right\|^2\\
%% \nonumber          &= \begin{pmatrix} 100(x_1 - y_1)\\ x_2 - y_2\end{pmatrix}^T\begin{pmatrix} 100(x_1 - y_1)\\ x_2 - y_2\end{pmatrix}\\
%%             &= (100(x_1 - y_1))^2 + (x_2 - y_2)^2 \label{eq:c1}.
%% \end{align}

%% From \eqref{eq:c1} we observe that $(x_1 - y_1)^2 \geq 0$ and
%% similarly $(x_2 - y_2)^2 \geq 0$ for all values in $\mathbb{R}$, so
%% our first criteria for a metric is fulfilled. We also observe that if
%% $d(\vect{x},\vect{y}) = 0$ it implies $x_1 - y_1 = 0$ and $x_2 - y_2 =
%% 0$, which means that $x_1 = y_1$ and $x_2 = y_2$, thus $d$ fulfills
%% our second requirement for a metric.

%% Our requirement of symmetry requires a little more
%% investigation. Proceeding from \eqref{eq:c1}, we find
%% \begin{align}
%% \nonumber (100(x_1 - y_1))^2 + (x_2 - y_2)^2 &= 100^2(x_1^2 - 2x_1y_1 + y_1^2) + (x_2^2 - 2x_2y_2 + y_2^2)\\
%%   \nonumber &= 100^2(y_1^2 - 2y_1x_1 + x_1^2) + (y_2^2 - 2y_2x_2 + x_2^2)\\
%%   \nonumber &= (100(y_1 - x_1))^2 + (y_2 - x_2)^2\\
%%   &= d(\vect{y},\vect{x})^2\label{eq:c2},
%% \end{align}
%% where \eqref{eq:c2} gives us $d(\vect{x},\vect{y}) =
%% d(\vect{y},\vect{x})$ and therefore fulfills the symmetry
%% requirement.

%% Our last requirement is the triangle inequality, i.e.\ $d(\vect{x},\vect{z}) \leq d(\vect{x},\vect{y}) + d(\vect{y},\vect{z}), \forall \vect{x},\vect{y},\vect{z}\in \mathbb{R}^2$.

%% Let $\vect{x},\vect{y},\vect{z}\in\mathbb{R}^2$.
%% \begin{align}
%%   d(\vect{x},\vect{z}) &= \left\|\vect{M}\vect{x} - \vect{M}\vect{z}\right\|\\
%%   &= \left\|\vect{M}\vect{z} - \vect{M}\vect{y} + \vect{M}\vect{y} - \vect{M}\vect{z}\right\|\\
%%   &\leq \left\|\vect{M}\vect{z} - \vect{M}\vect{y}\right\| + \left\|\vect{M}\vect{y} - \vect{M}\vect{z}\right\|\\
%%   &= d(\vect{x},\vect{y}) + d(\vect{y},\vect{z})
%%   %% &= (\vect{M}\vect{x} - \vect{M}\vect{z})^T(\vect{M}\vect{x} - \vect{M}\vect{z})\\
%%   %% &= ((\vect{M}\vect{x})^T - (\vect{M}\vect{z})^T)(\vect{M}\vect{x} - \vect{M}\vect{z})\\
%%   %% &= (\vect{M}\vect{x})^T\vect{M}\vect{x} \\
%%   %% &= \quad\vdots\quad \text{(We want to arrive at the following)} \\
%%   %% &\leq \left\|\vect{M}\vect{x} - \vect{M}\vect{y}\right\|^2 + \left\|\vect{M}\vect{y} - \vect{M}\vect{z}\right\|^2\\
%%   %% &= d(\vect{x},\vect{y})^2 + d(\vect{y},\vect{z})^2
%% \end{align}

\subsubsection{Results}
Using $d$ as metric on \knollC we obtain the results shown in table~\ref{tbl:knollCd}.

\begin{table}[h!]
  \centering
  \begin{tabular}{c | c|c }
    $k$ & training set & test set\\ \hline
    1 & 0.04 &   - \\
    3 & 0.02 & 0.02\\
    5 & 0.03 & 0.03\\ 
    7 & 0.04 & 0.04\\ 
    9 & 0.03 & 0.03 \\\hline\hline
    \hfill & 3 & 3
  \end{tabular}
  \caption{Results for \knollC using $d$ as metric}
  \label{tbl:knollCd}
\end{table}

The error rates drop dramatically. We could visualize with a plot?


\end{document}
